# Deep Learning 第四章の要約
## 概要
 これは[Deep Learning(Goodfellow)](http://www.deeplearningbook.org/)の第四章の要約である.
 
## 要約
### ■4.1 オーバーフローとアンダーフロー
- オーバーフロー  
 無限大をNaNとか9999999999にしちゃうことで発生するバグや誤差.
- アンダーフロー  
 0.000000001とかを0にしちゃうことで発生する誤差.
 
 
### ■4.2 悪条件
- 条件数(condition number)  
  コンピュータの計算において数値解析のしやすさを表したもので, 値が小さければ良条件で大きければ悪条件.
  
- 定義  
<img src="https://latex.codecogs.com/gif.latex?\kappa(\mathbf{a})&space;=&space;max_i_j|\frac{\lambda_i}{\lambda_j}|">  
ただし`λ`は`A`の固有値  


- 導出  
　<img src="https://latex.codecogs.com/gif.latex?\boldsymbol{Ax=b}">  
を解きたいとし, この場合`b`の変化によって解である`x`が変化する率が条件数となる.  
`b`の誤差を`e`をすると解に現れる誤差は,  
　<img src="https://latex.codecogs.com/gif.latex?\boldsymbol{A^{-1}e}">  
となる. 解の相対誤差と`b`の相対誤差の比率は  
　<img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/2ca16d17b71cd68866c440f1570b5d6a3ddcf7c7">  
これは次のように書き変えられる.  
　<img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/bbc5a312d28f4164729fcadb782ade45cb713aed">  
その最大値は2つの作用素ノルムの積となる.  
　<img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/da986df19c176694bafcb2ddafb2bbfc176d7543">  
当然, 行列ノルムの選択に依存しており, l2ノルムなら  
　<img src="https://latex.codecogs.com/gif.latex?\kappa(\mathbf{A})&space;=&space;max_i_j|\frac{\lambda_i}{\lambda_j}|">  
となる.  

### 4.3 勾配に基づく最適化
- 目的  
あたえられた関数f(x)の最小値を見つける.  

- 用語  

|用語|英語|説明|
|:--|:--|:--|
|最大値|global maximum|関数全体で最も大きな値|
|最小値|global minimum|関数全体で最も小さな値|
|極大値|local maximum|近傍で最も大きな値|
|極小値|local minimum|近傍で最も小さな値|
|臨界点|critical points|f'=0となる点|
|停留点|stationary points|同上|
|鞍点|saddle points|臨界点であるが極大値でも極小値でもない点|

- 勾配降下法  
`x′ = x − ε∇xf(x)`  
で新しい点を更新し続ける.  
εは学習率と呼ばれ, 正のスカラー値.  
値の決め方はいくつかあり,  
 - 小さい値にする
 - いくつかのεに対して計算し, 最も良かった値を採用する
などである.

### ■ 4.3.1 勾配を超えて : ヤコビ行列とヘッセ行列
- ヤコビ行列  
 <img src="https://latex.codecogs.com/png.latex?\dpi{100}&space;\bold{f}&space;:&space;\mathbb{R}^m&space;\rightarrow&space;\mathbb{R}^n">

参考 : [Wikipedia](https://ja.wikipedia.org/wiki/%E6%9D%A1%E4%BB%B6%E6%95%B0)  
